{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NRSFV3jujQte"
   },
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from scipy.signal import welch, find_peaks, periodogram\n",
    "from scipy.signal import lfilter\n",
    "from scipy.linalg import toeplitz, solve_toeplitz\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pyswarms as ps\n",
    "from tqdm.notebook import  tqdm\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tqF-OqCQjicv"
   },
   "outputs": [],
   "source": [
    "data = loadmat(\"Project_data.mat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6UjOsP0kkov3",
    "outputId": "1a0f112a-5040-4de3-bd14-b1b79d9caced"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'Channels', 'TestData', 'TrainData', 'TrainLabels', 'fs'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C5ZGVuGykwNH",
    "outputId": "58a82222-27e4-4c4a-a082-b1eb934a179f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162250000\n",
      "59\n",
      "5000\n",
      "550\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(data['TrainData'].size )\n",
    "\n",
    "print(len(data['TrainData']))\n",
    "\n",
    "print(len(data['TrainData'][1]) )\n",
    "\n",
    "print(data['TrainData'][1][1].size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nYZFW5fokwP_",
    "outputId": "cb11c585-458e-40a6-b5cf-fa512eda2063"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1, -1,  1, -1, -1,  1,  1,  1,  1,  1,  1, -1,  1, -1,  1,\n",
       "         1, -1, -1,  1,  1,  1,  1, -1, -1, -1,  1,  1, -1, -1,  1,  1,\n",
       "        -1, -1,  1,  1,  1,  1,  1, -1, -1,  1,  1, -1, -1, -1, -1,  1,\n",
       "        -1,  1, -1, -1, -1,  1,  1, -1, -1, -1,  1,  1, -1,  1,  1,  1,\n",
       "         1,  1, -1, -1, -1, -1, -1,  1,  1,  1,  1, -1, -1, -1,  1, -1,\n",
       "        -1,  1,  1, -1,  1, -1, -1, -1,  1, -1,  1, -1, -1,  1,  1, -1,\n",
       "         1,  1,  1,  1, -1,  1,  1, -1, -1, -1, -1, -1, -1,  1,  1, -1,\n",
       "        -1,  1, -1,  1,  1,  1, -1, -1,  1, -1,  1,  1, -1,  1, -1,  1,\n",
       "         1, -1, -1, -1,  1, -1,  1,  1,  1,  1,  1, -1, -1, -1,  1,  1,\n",
       "         1, -1,  1, -1,  1, -1,  1,  1,  1, -1, -1, -1, -1,  1,  1, -1,\n",
       "        -1,  1,  1, -1, -1, -1,  1, -1,  1,  1,  1,  1,  1,  1,  1, -1,\n",
       "        -1, -1, -1,  1, -1, -1,  1,  1,  1, -1,  1, -1,  1, -1, -1,  1,\n",
       "         1, -1, -1, -1,  1, -1, -1, -1,  1,  1,  1, -1, -1, -1,  1, -1,\n",
       "        -1,  1, -1, -1,  1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1,\n",
       "         1, -1,  1,  1,  1,  1,  1,  1, -1, -1,  1, -1,  1,  1, -1,  1,\n",
       "        -1, -1, -1,  1, -1, -1, -1,  1,  1, -1,  1,  1,  1, -1,  1,  1,\n",
       "         1,  1,  1, -1, -1, -1, -1, -1, -1, -1,  1,  1,  1,  1, -1, -1,\n",
       "        -1,  1, -1,  1, -1, -1, -1,  1,  1,  1,  1,  1,  1, -1,  1,  1,\n",
       "        -1,  1, -1,  1,  1, -1,  1,  1, -1, -1,  1, -1,  1, -1,  1, -1,\n",
       "         1,  1, -1, -1, -1,  1,  1, -1,  1,  1,  1, -1,  1, -1, -1, -1,\n",
       "        -1,  1, -1, -1, -1,  1,  1,  1, -1,  1, -1,  1, -1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1, -1, -1, -1, -1,  1, -1,  1, -1, -1,  1,  1,\n",
       "         1, -1, -1, -1, -1,  1, -1, -1,  1, -1,  1,  1,  1,  1,  1, -1,\n",
       "        -1, -1, -1,  1, -1,  1, -1,  1,  1, -1, -1,  1, -1,  1,  1,  1,\n",
       "        -1, -1,  1,  1,  1,  1, -1, -1,  1,  1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1,  1,  1,  1, -1, -1,  1,  1, -1,  1,  1,  1,  1, -1, -1,\n",
       "         1,  1, -1,  1,  1,  1, -1, -1, -1,  1,  1,  1,  1, -1, -1, -1,\n",
       "        -1, -1, -1, -1,  1,  1,  1, -1,  1,  1,  1, -1, -1, -1, -1, -1,\n",
       "        -1,  1, -1, -1, -1,  1, -1,  1,  1, -1, -1, -1,  1, -1,  1,  1,\n",
       "        -1,  1,  1, -1,  1,  1, -1,  1,  1, -1,  1,  1, -1, -1, -1,  1,\n",
       "         1, -1,  1, -1, -1,  1, -1, -1,  1, -1,  1,  1, -1, -1,  1, -1,\n",
       "        -1,  1,  1, -1,  1, -1,  1,  1, -1, -1, -1,  1, -1,  1,  1, -1,\n",
       "         1, -1, -1,  1, -1,  1, -1, -1,  1,  1,  1,  1, -1,  1,  1, -1,\n",
       "        -1,  1,  1,  1,  1, -1,  1, -1,  1, -1, -1, -1, -1, -1, -1,  1,\n",
       "         1,  1,  1, -1, -1,  1]], dtype=int16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['TrainLabels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking for balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3g_TgB4YkwTV",
    "outputId": "336ff79f-fe6f-44db-bdd3-1f8b6e078dfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array Shape: (1, 550)\n",
      "Total Number of Elements: 550\n",
      "Data Type of Elements: int16\n",
      "Mean: 0.007272727272727273\n",
      "Standard Deviation: 0.9999735533692962\n",
      "Count of -1: 273\n",
      "Count of  1: 277\n"
     ]
    }
   ],
   "source": [
    "array = data['TrainLabels']\n",
    "\n",
    "shape = array.shape\n",
    "size = array.size\n",
    "data_type = array.dtype\n",
    "mean = array.mean()\n",
    "std_dev = array.std()\n",
    "min_value = array.min()\n",
    "max_value = array.max()\n",
    "\n",
    "print(f\"Array Shape: {shape}\")\n",
    "print(f\"Total Number of Elements: {size}\")\n",
    "print(f\"Data Type of Elements: {data_type}\")\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Standard Deviation: {std_dev}\")\n",
    "\n",
    "count_neg_1 = np.sum(array == -1)\n",
    "count_pos_1 = np.sum(array == 1)\n",
    "\n",
    "print(\"Count of -1:\", count_neg_1)\n",
    "print(\"Count of  1:\", count_pos_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "S270GCyEkwWN"
   },
   "outputs": [],
   "source": [
    "\n",
    "def calculate_variance(signal):\n",
    "    return np.var(signal)\n",
    "\n",
    "def calculate_dominant_frequency_histogram(signal, fs):\n",
    "    frequencies, power = periodogram(signal, fs)\n",
    "    peaks, _ = find_peaks(power)\n",
    "    dominant_frequencies = frequencies[peaks]\n",
    "    histogram, bin_edges = np.histogram(dominant_frequencies, bins=10, range=(0, fs/2))\n",
    "    return histogram\n",
    "\n",
    "def calculate_standard_histogram(signal, bin_count=10):\n",
    "    histogram, bin_edges = np.histogram(signal, bins=bin_count)\n",
    "    return histogram\n",
    "\n",
    "def calculate_ar_model_coefficients(signal, order=4):\n",
    "    R = np.correlate(signal, signal, mode='full')[len(signal)-1:]\n",
    "    R /= R[0]\n",
    "    ar_coeffs = solve_toeplitz((R[:order], R[:order]), R[1:order+1])\n",
    "    return ar_coeffs\n",
    "\n",
    "def calculate_form_factor(signal):\n",
    "    rms = np.sqrt(np.mean(signal**2))\n",
    "    mean_abs = np.mean(np.abs(signal))\n",
    "    return rms / mean_abs\n",
    "\n",
    "def calculate_covariance_between_channels(signal1, signal2):\n",
    "    return np.cov(signal1, signal2)[0, 1]\n",
    "\n",
    "def calculate_correlation_between_channels(signal1, signal2):\n",
    "    return np.corrcoef(signal1, signal2)[0, 1]\n",
    "\n",
    "def calculate_frequency_features(signal, fs):\n",
    "    freqs, power = periodogram(signal, fs)\n",
    "    max_freq = freqs[np.argmax(power)]\n",
    "    mean_freq = np.sum(freqs * power) / np.sum(power)\n",
    "    median_freq = freqs[np.searchsorted(np.cumsum(power), np.sum(power)/2)]\n",
    "    return max_freq, mean_freq, median_freq\n",
    "\n",
    "def calculate_relative_band_power(signal, fs):\n",
    "    bands = {'Delta': (0.5, 4), 'Theta': (4, 8), 'Alpha': (8, 12), 'Beta': (12, 30), 'Gamma': (30, 45)}\n",
    "    freqs, power = periodogram(signal, fs)\n",
    "    total_power = np.sum(power)\n",
    "    relative_powers = {}\n",
    "    for band in bands:\n",
    "        band_freqs = bands[band]\n",
    "        idx_band = np.logical_and(freqs >= band_freqs[0], freqs <= band_freqs[1])\n",
    "        relative_powers[band] = np.sum(power[idx_band]) / total_power\n",
    "    return relative_powers\n",
    "\n",
    "\n",
    "\n",
    "# channel_num = signal.shape[0]\n",
    "# train_num = signal.shape[2]\n",
    "\n",
    "# for i in train_num\n",
    "# variance = calculate_variance(signal)\n",
    "# dom_freq_hist = calculate_dominant_frequency_histogram(signal, fs)\n",
    "# std_hist = calculate_standard_histogram(signal)\n",
    "# ar_coeffs = calculate_ar_model_coefficients(signal)\n",
    "# form_factor = calculate_form_factor(signal)\n",
    "# covariance = calculate_covariance_between_channels(signal, data['TrainData'][1][0])\n",
    "# correlation = calculate_correlation_between_channels(signal, data['TrainData'][1][0])\n",
    "# max_freq, mean_freq, median_freq = calculate_frequency_features(signal, fs)\n",
    "# relative_band_power = calculate_relative_band_power(signal, fs)\n",
    "\n",
    "def calculate_covariance_between_channels_1(chnl_signal, exp_signal):\n",
    "    cov_list = []\n",
    "    for i, signal in enumerate(exp_signal):\n",
    "        cov_list.append(calculate_covariance_between_channels(chnl_signal, signal))\n",
    "    return cov_list\n",
    "\n",
    "def calculate_correlation_between_channels_1(chnl_signal, exp_signal):\n",
    "    cor_list = []\n",
    "    for i, signal in enumerate(exp_signal):\n",
    "        cor_list.append(calculate_correlation_between_channels(chnl_signal, signal))\n",
    "    return cor_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define feature function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "qTA8Ee5mkwZL"
   },
   "outputs": [],
   "source": [
    "def extract_feature(signal, set_name='features'):\n",
    "    channel_num = signal.shape[0]\n",
    "    train_num = signal.shape[2]\n",
    "    all_experiments_features = [] \n",
    "    \n",
    "    for exp_idx in tqdm(range(train_num)):#train_num\n",
    "        experiment_signal = signal[:,:, exp_idx]\n",
    "        # print(experiment_signal.shape)\n",
    "        experiment_features = []  \n",
    "        \n",
    "        for i, channel_signal in enumerate(experiment_signal):  \n",
    "            covariance = calculate_covariance_between_channels_1(channel_signal, experiment_signal)\n",
    "            correlation = calculate_correlation_between_channels_1(channel_signal, experiment_signal)\n",
    "            max_freq, mean_freq, median_freq = calculate_frequency_features(channel_signal, fs)    \n",
    "            relative_band_power = calculate_relative_band_power(channel_signal, fs)\n",
    "            variance = calculate_variance(channel_signal)\n",
    "            dom_freq_hist = calculate_dominant_frequency_histogram(channel_signal, fs)\n",
    "            std_hist = calculate_standard_histogram(channel_signal)\n",
    "            ar_coeffs = calculate_ar_model_coefficients(channel_signal)\n",
    "            form_factor = calculate_form_factor(channel_signal)\n",
    "            # print(form_factor)\n",
    "        \n",
    "            channel_features = [\n",
    "                variance, *dom_freq_hist, *std_hist, *ar_coeffs, form_factor, *covariance, *correlation,\n",
    "                max_freq, mean_freq, median_freq, *relative_band_power.values()\n",
    "            ]\n",
    "            # print(len(channel_features))\n",
    "            experiment_features.extend(channel_features)\n",
    "    \n",
    "        all_experiments_features.append(experiment_features)\n",
    "        \n",
    "    with open(f'{set_name}.pkl', 'wb') as f:\n",
    "        pickle.dump(all_experiments_features, f)\n",
    "        \n",
    "    return all_experiments_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RQWZBaPik50Q",
    "outputId": "d9f17a9f-ebbb-45e2-92f4-08f0c8118408"
   },
   "outputs": [],
   "source": [
    "# len(all_experiments_features)/550\n",
    "# all_experiments_features_ = []\n",
    "# for i in range(550):\n",
    "#     list_ = all_experiments_features[8968*i:8968*(i+1)]\n",
    "#     all_experiments_features_.append(list_)\n",
    "# len(all_experiments_features_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ngH1cRA22MqR",
    "outputId": "c3e45ab3-7503-4dd4-f31b-a8355fbd9585"
   },
   "outputs": [],
   "source": [
    "def normalize_feature(all_experiments_features, scaler=None):\n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    all_experiments_features_imputed = imputer.fit_transform(all_experiments_features)\n",
    "\n",
    "    if not scaler:\n",
    "        # print(\"Train data Normalization\")\n",
    "        scaler = StandardScaler()\n",
    "    all_experiments_features_normalized = scaler.fit_transform(all_experiments_features_imputed)\n",
    "    \n",
    "    all_experiments_features_non_negative = all_experiments_features_normalized - np.min(all_experiments_features_normalized)\n",
    "\n",
    "    return all_experiments_features_non_negative, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_feature(all_experiments_features, labels, k=50):\n",
    "    selector = SelectKBest(chi2, k=k)\n",
    "    selected_features = selector.fit_transform(all_experiments_features, labels)\n",
    "    \n",
    "    selected_indices = selector.get_support(indices=True)\n",
    "    return selected_features, selected_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Data and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00fe501a562e43f28b68bafa6d987285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/550 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size:      (59, 5000, 550) -> (550, 50)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5edf89d3ad94a048e655f5042a1ec8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size:       (59, 5000, 159) -> (159, 50)\n"
     ]
    }
   ],
   "source": [
    "fs = 1000 \n",
    "\n",
    "train_signal = data['TrainData']\n",
    "train_labels = np.array(data['TrainLabels']).flatten()\n",
    "train_labels[train_labels == -1] = 0\n",
    "\n",
    "# valid_train_signal = data['TrainData']\n",
    "# valid_train_labels = np.array(data['TrainLabels']).flatten()\n",
    "# valid_train_labels[valid_train_labels == -1] = 0\n",
    "# random_nums = np.random.permutation(valid_train_signal.shape[2])\n",
    "# threshold = int(0.1 * valid_train_signal.shape[2])\n",
    "\n",
    "\n",
    "# train_signal = valid_train_signal[:,:, random_nums[threshold:]]\n",
    "# train_labels = valid_train_labels[random_nums[threshold:]]\n",
    "# valid_signal = valid_train_signal[:,:, random_nums[:threshold]]\n",
    "# valid_labels = valid_train_labels[random_nums[:threshold]]\n",
    "\n",
    "\n",
    "\n",
    "test_signal = data['TestData']\n",
    "\n",
    "# print(test_signal.shape)\n",
    "\n",
    "\n",
    "##-----------Train data-------------##\n",
    "try:\n",
    "    with open('train.pkl', 'rb') as f:\n",
    "        # valid_train_experiments_features = pickle.load(f)\n",
    "        train_experiments_features = pickle.load(f)\n",
    "except:\n",
    "    # valid_train_experiments_features = extract_feature(valid_train_signal, 'train')\n",
    "    train_experiments_features = extract_feature(train_signal, 'train')\n",
    "    \n",
    "# train_experiments_features = [valid_train_experiments_features[i] for i in random_nums[threshold:]]\n",
    "# valid_experiments_features = [valid_train_experiments_features[i] for i in random_nums[:threshold]]\n",
    "\n",
    "train_experiments_features_normalized, train_normalizer = normalize_feature(train_experiments_features)\n",
    "train_selected_features, train_selected_indices = select_feature(train_experiments_features_normalized, train_labels)\n",
    "print(f\"Train set size:      {train_signal.shape} -> {train_selected_features.shape}\")\n",
    "\n",
    "##-----------Validation data-------------##\n",
    "\n",
    "\n",
    "# valid_experiments_features_normalized, _ = normalize_feature(valid_experiments_features, train_normalizer)\n",
    "# valid_selected_features = valid_experiments_features_normalized[:, train_selected_indices]\n",
    "# print(f\"Validation set size: {valid_signal.shape}  -> {valid_selected_features.shape}\")\n",
    "\n",
    "##-----------Test data-------------##\n",
    "\n",
    "try:\n",
    "    with open('test.pkl', 'rb') as f:\n",
    "         test_experiments_features = pickle.load(f)\n",
    "except:\n",
    "    test_experiments_features = extract_feature(test_signal, 'test')\n",
    "test_experiments_features_normalized, _ = normalize_feature(test_experiments_features, train_normalizer)\n",
    "test_selected_features = test_experiments_features_normalized[:, train_selected_indices]\n",
    "print(f\"Test set size:       {test_signal.shape} -> {test_selected_features.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ybEffTj8g2k",
    "outputId": "ceb1f845-602f-4283-dbac-9174a5fbab24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.60080955,  6.42968057,  9.31057258, ...,  8.15404857,\n",
       "         7.01968903,  8.2802208 ],\n",
       "       [ 8.13949418,  8.6041973 ,  7.97247892, ..., 10.35589971,\n",
       "         7.92661266,  7.49003864],\n",
       "       [ 7.91629466,  9.53613304,  9.20764229, ...,  7.62805576,\n",
       "         8.85769378,  9.41511561],\n",
       "       ...,\n",
       "       [ 7.59240289,  8.91484254,  8.48713033, ...,  7.82971239,\n",
       "         7.2990854 ,  8.14991256],\n",
       "       [10.63672799,  8.91484254,  5.81094303, ...,  7.70551414,\n",
       "        10.2784142 ,  8.22882529],\n",
       "       [ 7.8016933 ,  7.67226156,  9.61936342, ...,  7.47161765,\n",
       "         8.4710336 ,  9.54921009]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_experiments_features_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4Bl3nVhAWRR"
   },
   "source": [
    "There is a [sourse on Kaggle](https:/https://www.kaggle.com/code/prashant111/comprehensive-guide-on-featur/)  that mentions the implementation of the chi-square test as an equivalent to Fisher's score for feature selection. This approach is often used in statistical analysis to determine the independence of two events. In the context of feature selection, it's used to evaluate whether the occurrence of a specific feature and the occurrence of a specific class are independent. This method can be particularly useful in categorical data analysis.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also use j fisher method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# all_experiments_features_normalized = scaler.fit_transform(all_experiments_features)\n",
    "\n",
    "\n",
    "# def calculate_fishers_score(features, labels):\n",
    "#     features_class_1 = features[labels == 1]\n",
    "#     features_class_0 = features[labels == 0]\n",
    "\n",
    "#     mean_1 = np.mean(features_class_1, axis=0)\n",
    "#     mean_0 = np.mean(features_class_0, axis=0)\n",
    "#     var_1 = np.var(features_class_1, axis=0)\n",
    "#     var_0 = np.var(features_class_0, axis=0)\n",
    "\n",
    "#     fishers_score = (mean_1 - mean_0) ** 2 / (var_1 + var_0)\n",
    "\n",
    "#     return fishers_score\n",
    "\n",
    "# fishers_scores = calculate_fishers_score(all_experiments_features_normalized, labels)\n",
    "# num_top_features = 50 \n",
    "# top_feature_indices = np.argsort(fishers_scores)[-num_top_features:]\n",
    "\n",
    "# selected_features_J = all_experiments_features_normalized[:, top_feature_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_features_df = pd.DataFrame(selected_features_J)\n",
    "# selected_features_df.to_csv('selected_features.csv', index=False)\n",
    "# selcted_features_count=pd.read_csv(\"selcted_features_count.csv\")\n",
    "# selcted_features_count.to_numpy().shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find best MLP Modele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer Sizes: (100, 100), Activation: relu\n",
      "Average Accuracy: 0.7872727272727273\n",
      "Hidden Layer Sizes: (100, 100), Activation: logistic\n",
      "Average Accuracy: 0.7745454545454546\n",
      "Hidden Layer Sizes: (100, 100), Activation: tanh\n",
      "Average Accuracy: 0.7890909090909091\n",
      "Hidden Layer Sizes: (150, 100), Activation: relu\n",
      "Average Accuracy: 0.8018181818181818\n",
      "Hidden Layer Sizes: (150, 100), Activation: logistic\n",
      "Average Accuracy: 0.7836363636363637\n",
      "Hidden Layer Sizes: (150, 100), Activation: tanh\n",
      "Average Accuracy: 0.7890909090909092\n",
      "Hidden Layer Sizes: (160, 100), Activation: relu\n",
      "Average Accuracy: 0.801818181818182\n",
      "Hidden Layer Sizes: (160, 100), Activation: logistic\n",
      "Average Accuracy: 0.7763636363636364\n",
      "Hidden Layer Sizes: (160, 100), Activation: tanh\n",
      "Average Accuracy: 0.7745454545454545\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(160, 100), max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(160, 100), max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(160, 100), max_iter=1000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "selected_features_normalized = scaler.fit_transform(train_selected_features)\n",
    "\n",
    "hidden_layer_sizes_list = [ (100, 100), (150, 100), (160, 100)]  \n",
    "activation_functions = ['relu', 'logistic', 'tanh']  \n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "\n",
    "for hidden_layer_sizes in hidden_layer_sizes_list:\n",
    "    for activation in activation_functions:\n",
    "       \n",
    "        mlp = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, activation=activation, max_iter=1000)\n",
    "\n",
    "       \n",
    "        kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        accuracies = cross_val_score(mlp, selected_features_normalized, train_labels, cv=kfold, scoring='accuracy')\n",
    "\n",
    " \n",
    "        avg_accuracy = np.mean(accuracies)\n",
    "\n",
    "        print(f\"Hidden Layer Sizes: {hidden_layer_sizes}, Activation: {activation}\")\n",
    "        print(f\"Average Accuracy: {avg_accuracy}\")\n",
    "\n",
    "        \n",
    "        if avg_accuracy > best_accuracy:\n",
    "            best_accuracy = avg_accuracy\n",
    "            best_params['hidden_layer_sizes'] = hidden_layer_sizes\n",
    "            best_params['activation'] = activation\n",
    "\n",
    "best_mlp = MLPClassifier(hidden_layer_sizes=best_params['hidden_layer_sizes'],\n",
    "                         activation=best_params['activation'], max_iter=1000)\n",
    "best_mlp.fit(selected_features_normalized, train_labels)\n",
    "\n",
    "# best_mlp.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find feature for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "       1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 0, 0, 1, 0], dtype=int16)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "selected_features_normalized = scaler.fit_transform(test_selected_features)\n",
    "best_mlp.predict(selected_features_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import savemat\n",
    "predicted_labels = best_mlp.predict(selected_features_normalized)\n",
    "predicted_labels[predicted_labels == 0] = -1\n",
    "\n",
    "savemat('mlp_test.mat', {'predicted_labels': predicted_labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  find best RBF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Clusters: 25\n",
      "Average Accuracy (RBF Network): 0.6927272727272726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Clusters: 10\n",
      "Average Accuracy (RBF Network): 0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Clusters: 15\n",
      "Average Accuracy (RBF Network): 0.709090909090909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Clusters: 20\n",
      "Average Accuracy (RBF Network): 0.7018181818181818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scaler = StandardScaler()\n",
    "selected_features_normalized = scaler.fit_transform(train_selected_features)\n",
    "\n",
    "hidden_layer_sizes_rbf = (5, 4)  \n",
    "n_clusters = 10 \n",
    "\n",
    "best_accuracy_rbf = 0\n",
    "best_params_rbf = {}\n",
    "\n",
    "for n_clusters in [25, 10, 15,20]:  \n",
    "    rbf_pipeline = make_pipeline(\n",
    "        KMeans(n_clusters=n_clusters),\n",
    "        MLPClassifier(hidden_layer_sizes=hidden_layer_sizes_rbf, activation='identity', max_iter=1000)\n",
    "    )\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    accuracies_rbf = cross_val_score(rbf_pipeline, selected_features_normalized, train_labels, cv=kfold, scoring='accuracy')\n",
    "\n",
    "    avg_accuracy_rbf = np.mean(accuracies_rbf)\n",
    "\n",
    "    print(f\"Number of Clusters: {n_clusters}\")\n",
    "    print(f\"Average Accuracy (RBF Network): {avg_accuracy_rbf}\")\n",
    "\n",
    "    if avg_accuracy_rbf > best_accuracy_rbf:\n",
    "        best_accuracy_rbf = avg_accuracy_rbf\n",
    "        best_params_rbf['n_clusters'] = n_clusters\n",
    "\n",
    "best_rbf_pipeline = make_pipeline(\n",
    "    KMeans(n_clusters=best_params_rbf['n_clusters']),\n",
    "    MLPClassifier(hidden_layer_sizes=hidden_layer_sizes_rbf, activation='identity', max_iter=1000)\n",
    ")\n",
    "best_rbf_pipeline.fit(selected_features_normalized, train_labels)\n",
    "\n",
    "best_rbf = best_rbf_pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find feature for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "       1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0], dtype=int16)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "selected_features_normalized = scaler.fit_transform(test_selected_features)\n",
    "best_rbf_pipeline.predict(selected_features_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import savemat\n",
    "predicted_labels = best_rbf_pipeline.predict(selected_features_normalized)\n",
    "predicted_labels[predicted_labels == 0] = -1\n",
    "\n",
    "savemat('rbf_test.mat', {'predicted_labels': predicted_labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PSO function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59, 5000, 550)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a674b58093744368d41d873558fba13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/550 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def calculate_variance(signal):\n",
    "    return np.var(signal)\n",
    "\n",
    "def calculate_dominant_frequency_histogram(signal, fs):\n",
    "    frequencies, power = periodogram(signal, fs)\n",
    "    peaks, _ = find_peaks(power)\n",
    "    dominant_frequencies = frequencies[peaks]\n",
    "    histogram, bin_edges = np.histogram(dominant_frequencies, bins=10, range=(0, fs/2))\n",
    "    return histogram\n",
    "\n",
    "def calculate_standard_histogram(signal, bin_count=10):\n",
    "    histogram, bin_edges = np.histogram(signal, bins=bin_count)\n",
    "    return histogram\n",
    "\n",
    "def calculate_ar_model_coefficients(signal, order=4):\n",
    "    R = np.correlate(signal, signal, mode='full')[len(signal)-1:]\n",
    "    R /= R[0]\n",
    "    ar_coeffs = solve_toeplitz((R[:order], R[:order]), R[1:order+1])\n",
    "    return ar_coeffs\n",
    "\n",
    "def calculate_form_factor(signal):\n",
    "    rms = np.sqrt(np.mean(signal**2))\n",
    "    mean_abs = np.mean(np.abs(signal))\n",
    "    return rms / mean_abs\n",
    "\n",
    "def calculate_covariance_between_channels(signal1, signal2):\n",
    "    return np.cov(signal1, signal2)[0, 1]\n",
    "\n",
    "def calculate_correlation_between_channels(signal1, signal2):\n",
    "    return np.corrcoef(signal1, signal2)[0, 1]\n",
    "\n",
    "def calculate_frequency_features(signal, fs):\n",
    "    freqs, power = periodogram(signal, fs)\n",
    "    max_freq = freqs[np.argmax(power)]\n",
    "    mean_freq = np.sum(freqs * power) / np.sum(power)\n",
    "    median_freq = freqs[np.searchsorted(np.cumsum(power), np.sum(power)/2)]\n",
    "    return max_freq, mean_freq, median_freq\n",
    "\n",
    "def calculate_relative_band_power(signal, fs):\n",
    "    bands = {'Delta': (0.5, 4), 'Theta': (4, 8), 'Alpha': (8, 12), 'Beta': (12, 30), 'Gamma': (30, 45)}\n",
    "    freqs, power = periodogram(signal, fs)\n",
    "    total_power = np.sum(power)\n",
    "    relative_powers = {}\n",
    "    for band in bands:\n",
    "        band_freqs = bands[band]\n",
    "        idx_band = np.logical_and(freqs >= band_freqs[0], freqs <= band_freqs[1])\n",
    "        relative_powers[band] = np.sum(power[idx_band]) / total_power\n",
    "    return relative_powers\n",
    "\n",
    "fs = 1000 \n",
    "signal = data['TrainData']\n",
    "print(signal.shape)\n",
    "\n",
    "channel_num = signal.shape[0]\n",
    "train_num = signal.shape[2]\n",
    "\n",
    "\n",
    "def calculate_covariance_between_channels_1(chnl_signal, exp_signal):\n",
    "    cov_list = []\n",
    "    for i, signal in enumerate(exp_signal):\n",
    "        cov_list.append(calculate_covariance_between_channels(chnl_signal, signal))\n",
    "    return cov_list\n",
    "\n",
    "def calculate_correlation_between_channels_1(chnl_signal, exp_signal):\n",
    "    cor_list = []\n",
    "    for i, signal in enumerate(exp_signal):\n",
    "        cor_list.append(calculate_correlation_between_channels(chnl_signal, signal))\n",
    "    return cor_list\n",
    "channel_num = signal.shape[0]\n",
    "train_num = signal.shape[2]\n",
    "\n",
    "all_experiments_features = [] \n",
    "\n",
    "for exp_idx in tqdm(range(train_num)):#train_num\n",
    "    experiment_signal = signal[:,:, exp_idx]\n",
    "    # print(experiment_signal.shape)\n",
    "    experiment_features = []  \n",
    "    \n",
    "    for i, channel_signal in enumerate(experiment_signal):  \n",
    "        covariance = calculate_covariance_between_channels_1(channel_signal, experiment_signal)\n",
    "        correlation = calculate_correlation_between_channels_1(channel_signal, experiment_signal)\n",
    "        max_freq, mean_freq, median_freq = calculate_frequency_features(channel_signal, fs)    \n",
    "        relative_band_power = calculate_relative_band_power(channel_signal, fs)\n",
    "        variance = calculate_variance(channel_signal)\n",
    "        dom_freq_hist = calculate_dominant_frequency_histogram(channel_signal, fs)\n",
    "        std_hist = calculate_standard_histogram(channel_signal)\n",
    "        ar_coeffs = calculate_ar_model_coefficients(channel_signal)\n",
    "        form_factor = calculate_form_factor(channel_signal)\n",
    "        # print(form_factor)\n",
    "    \n",
    "        channel_features = [\n",
    "            variance, *dom_freq_hist, *std_hist, *ar_coeffs, form_factor, *covariance, *correlation,\n",
    "            max_freq, mean_freq, median_freq, *relative_band_power.values()\n",
    "        ]\n",
    "        # print(len(channel_features))\n",
    "        experiment_features.extend(channel_features)\n",
    "\n",
    "    all_experiments_features.append(experiment_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PSO_feature_selection(all_features, n_features):\n",
    "    n_particles = 20\n",
    "    x_swarm = np.zeros((n_particles, n_features), dtype=int)\n",
    "\n",
    "    n_all_features = all_features.shape[2]\n",
    "\n",
    "    for i in range(n_particles):\n",
    "        x_swarm[i, :] = np.random.permutation(n_all_features)[:n_features]\n",
    "\n",
    "    all_j_scores = []\n",
    "    current_j_score = np.zeros(n_particles)\n",
    "\n",
    "    for i in range(n_particles):\n",
    "        group_1 = all_features[0, :, x_swarm[i, :]]\n",
    "        group_2 = all_features[1, :, x_swarm[i, :]]\n",
    "        both_groups = np.concatenate((group_1, group_2))\n",
    "        current_j_score[i] = j_score_cal_function(group_1, group_2, both_groups)\n",
    "\n",
    "    all_j_scores.append(current_j_score)\n",
    "\n",
    "    x_best_local = np.copy(x_swarm)\n",
    "    x_best_global = x_swarm[np.argmax(current_j_score)]\n",
    "    max_j = np.max(current_j_score)\n",
    "    local_max_j = np.copy(current_j_score)\n",
    "\n",
    "    alpha = 1\n",
    "    t = 1\n",
    "    b1 = np.random.rand()\n",
    "    b2 = np.random.rand()\n",
    "    v = np.zeros((n_particles, n_features, 2), dtype=float)\n",
    "    J_thresh = 0.08\n",
    "\n",
    "    while (np.sum(all_j_scores[-1] >= J_thresh) < n_particles) or (t < 100):\n",
    "        for i in range(n_particles):\n",
    "            v[i, :, 1] = alpha * v[i, :, 0] + b1 * (x_best_local[i, :] - x_swarm[i, :]) + b2 * (x_best_global - x_swarm[i, :])\n",
    "            x_swarm[i, :] = np.round(x_swarm[i, :] + v[i, :, 1])\n",
    "\n",
    "            if len(np.unique(x_swarm[i, :])) != n_features:\n",
    "                x_swarm[i, :] = np.round(x_swarm[i, :] - 0.5 * np.random.rand())\n",
    "\n",
    "            n_all_features = all_features.shape[2]\n",
    "            upper_bound = x_swarm[i, :] > n_all_features\n",
    "            x_swarm[i, upper_bound] = n_all_features\n",
    "            lower_bound = x_swarm[i, :] < 1\n",
    "            x_swarm[i, lower_bound] = 1\n",
    "\n",
    "            group_1 = all_features[0, :, x_swarm[i, :]]\n",
    "            group_2 = all_features[1, :, x_swarm[i, :]]\n",
    "            both_groups = np.concatenate((group_1, group_2))\n",
    "            prev_j_score = current_j_score[i]\n",
    "            current_j_score[i] = j_score_cal_function(group_1, group_2, both_groups)\n",
    "\n",
    "            if current_j_score[i] >= local_max_j[i]:\n",
    "                x_best_local[i, :] = x_swarm[i, :]\n",
    "                local_max_j[i] = current_j_score[i]\n",
    "\n",
    "            if np.max(current_j_score) >= max_j:\n",
    "                max_j = np.max(current_j_score)\n",
    "                x_best_global = x_swarm[np.argmax(current_j_score), :]\n",
    "\n",
    "            t += 1\n",
    "            alpha = 1 / t\n",
    "            b1 = np.random.rand()\n",
    "            b2 = np.random.rand()\n",
    "\n",
    "        all_j_scores.append(current_j_score)\n",
    "\n",
    "    return x_best_global\n",
    "\n",
    "def j_score_cal_function(group_1, group_2, both_groups):\n",
    "\n",
    "    mean_group_1 = np.mean(group_1)\n",
    "    mean_group_2 = np.mean(group_2)\n",
    "    mean_both_groups = np.mean(both_groups)\n",
    "\n",
    "    var_between = len(group_1) * ((mean_group_1 - mean_both_groups) ** 2 + (mean_group_2 - mean_both_groups) ** 2)\n",
    "    var_within = np.sum((group_1 - mean_group_1) ** 2) + np.sum((group_2 - mean_group_2) ** 2)\n",
    "\n",
    "    j_score = var_between / (var_between + var_within)\n",
    "\n",
    "    return j_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-04 02:15:02,039 - pyswarms.single.global_best - INFO - Optimize for 100 iters with {'c1': 1.5, 'c2': 1.5, 'w': 0.9}\n",
      "pyswarms.single.global_best: 100%|██████████|100/100, best_cost=1\n",
      "2024-02-04 02:15:05,848 - pyswarms.single.global_best - INFO - Optimization finished | best cost: 1.0, best pos: [0.76658365 0.99887434 0.15989154 ... 0.77948393 0.19230361 0.52642582]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features shape: [[ 0.03437026  0.1838853  -1.69796564 ... -0.40576522  2.44438493\n",
      "   1.16489373]\n",
      " [-0.33033712 -0.68798465  1.02707378 ...  3.68548989 -2.48698859\n",
      "  -1.79009606]\n",
      " [-0.64094116 -2.43172455 -1.79889303 ... -0.51550472 -0.75709358\n",
      "   0.7974495 ]\n",
      " ...\n",
      " [ 2.34645109  0.76513193 -1.59703825 ... -0.46981193 -0.12589\n",
      "   0.21626472]\n",
      " [-0.28826638  0.1838853   0.52243685 ... -0.54895285  1.31773552\n",
      "   0.40280457]\n",
      " [ 2.02364041 -0.97860797 -1.79889303 ...  0.04226653  1.09117406\n",
      "  -0.07768295]]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "all_experiments_features_normalized = scaler.fit_transform(all_experiments_features)\n",
    "all_experiments_features_normalized.shape\n",
    "\n",
    "labels = np.array(data['TrainLabels'])\n",
    "labels[labels == -1] = 0\n",
    "labels.shape\n",
    "\n",
    "\n",
    "# def fitness_function(position):\n",
    "#     selected_features_bool = np.clip(position, 0, 1).astype(bool)\n",
    "    \n",
    "#     if len(selected_features_bool) == all_experiments_features_normalized.shape[1]:\n",
    "#         selected_features = all_experiments_features_normalized[:, selected_features_bool]\n",
    "#         classifier = SVC()\n",
    "#         scores = cross_val_score(classifier, selected_features, labels, cv=5)\n",
    "#         return 1 - scores.mean() \n",
    "#     else:\n",
    "#         return 1\n",
    "\n",
    "\n",
    "# options = {'c1': 1.5, 'c2': 1.5, 'w':0.9}\n",
    "\n",
    "# optimizer = ps.single.GlobalBestPSO(n_particles=50, dimensions=all_experiments_features_normalized.shape[1], options=options)\n",
    "\n",
    "# cost, pos = optimizer.optimize(fitness_function, iters=100)\n",
    "\n",
    "# selected_features_bool = np.clip(pos, 0, 1).astype(bool)\n",
    "\n",
    "# if len(selected_features_bool) == all_experiments_features_normalized.shape[1]:\n",
    "#     try:\n",
    "#         selected_features = all_experiments_features_normalized[:, selected_features_bool]\n",
    "#         print(\"Selected features shape:\", selected_features)\n",
    "#     except IndexError as e:\n",
    "#         print(\"IndexError:\", e)\n",
    "#         print(\"Further diagnostic needed.\")\n",
    "# else:\n",
    "#     print(\"Mismatch in dimensions! Boolean mask length does not match the number of features.\")\n",
    "\n",
    "\n",
    "\n",
    "# ... (your feature calculation functions)\n",
    "\n",
    "# Assuming 'all_experiments_features_normalized' is the array containing your features\n",
    "# and 'labels' is your array of labels\n",
    "\n",
    "def fitness_function(position):\n",
    "    selected_features_bool = np.clip(position, 0, 1).astype(bool)\n",
    "    \n",
    "    if len(selected_features_bool) == len(all_experiments_features_normalized[0]):\n",
    "        selected_features = all_experiments_features_normalized[:, selected_features_bool]\n",
    "        \n",
    "        # Initialize your classifier (e.g., SVC)\n",
    "        classifier = SVC()\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        scores = cross_val_score(classifier, selected_features, labels, cv=5)\n",
    "        \n",
    "        # Return 1 - mean accuracy as PSO minimizes the objective function\n",
    "        return 1 - scores.mean() \n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Set dimensions to the number of features before selection\n",
    "options = {'c1': 1.5, 'c2': 1.5, 'w': 0.9}\n",
    "optimizer = ps.single.GlobalBestPSO(n_particles=50, dimensions=len(all_experiments_features_normalized[0]), options=options)\n",
    "\n",
    "# Run PSO optimization\n",
    "cost, pos = optimizer.optimize(fitness_function, iters=100)\n",
    "\n",
    "# Extract selected features\n",
    "selected_features_bool = np.clip(pos, 0, 1).astype(bool)\n",
    "selected_features_PSO = all_experiments_features_normalized[:, selected_features_bool]\n",
    "print(\"Selected features shape:\", selected_features_PSO)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of selected_features_normalized: (550, 8968)\n",
      "Shape of labels: (1, 550)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of selected_features_normalized:\", selected_features_normalized.shape)\n",
    "print(\"Shape of labels:\", labels.shape)  # Add this line to check the shape of labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer Sizes: (100, 100), Activation: relu\n",
      "Average Accuracy: 0.8509090909090908\n",
      "Hidden Layer Sizes: (100, 100), Activation: logistic\n",
      "Average Accuracy: 0.8418181818181818\n",
      "Hidden Layer Sizes: (100, 100), Activation: tanh\n",
      "Average Accuracy: 0.8618181818181817\n",
      "Hidden Layer Sizes: (150, 100), Activation: relu\n",
      "Average Accuracy: 0.8527272727272728\n",
      "Hidden Layer Sizes: (150, 100), Activation: logistic\n",
      "Average Accuracy: 0.8509090909090908\n",
      "Hidden Layer Sizes: (150, 100), Activation: tanh\n",
      "Average Accuracy: 0.8727272727272728\n",
      "Hidden Layer Sizes: (160, 100), Activation: relu\n",
      "Average Accuracy: 0.8545454545454547\n",
      "Hidden Layer Sizes: (160, 100), Activation: logistic\n",
      "Average Accuracy: 0.841818181818182\n",
      "Hidden Layer Sizes: (160, 100), Activation: tanh\n",
      "Average Accuracy: 0.850909090909091\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, hidden_layer_sizes=(150, 100), max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, hidden_layer_sizes=(150, 100), max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(activation='tanh', hidden_layer_sizes=(150, 100), max_iter=1000)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "selected_features_normalized = scaler.fit_transform(selected_features_PSO)\n",
    "\n",
    "hidden_layer_sizes_list = [ (100, 100), (150, 100), (160, 100)]  \n",
    "activation_functions = ['relu', 'logistic', 'tanh']  \n",
    "labels = np.ravel(labels)\n",
    "\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "\n",
    "for hidden_layer_sizes in hidden_layer_sizes_list:\n",
    "    for activation in activation_functions:\n",
    "       \n",
    "        mlp = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, activation=activation, max_iter=1000)\n",
    "\n",
    "       \n",
    "        kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        accuracies = cross_val_score(mlp, selected_features_normalized, labels, cv=kfold, scoring='accuracy')\n",
    "\n",
    " \n",
    "        avg_accuracy = np.mean(accuracies)\n",
    "\n",
    "        print(f\"Hidden Layer Sizes: {hidden_layer_sizes}, Activation: {activation}\")\n",
    "        print(f\"Average Accuracy: {avg_accuracy}\")\n",
    "\n",
    "        \n",
    "        if avg_accuracy > best_accuracy:\n",
    "            best_accuracy = avg_accuracy\n",
    "            best_params['hidden_layer_sizes'] = hidden_layer_sizes\n",
    "            best_params['activation'] = activation\n",
    "\n",
    "best_mlp = MLPClassifier(hidden_layer_sizes=best_params['hidden_layer_sizes'],\n",
    "                         activation=best_params['activation'], max_iter=1000)\n",
    "best_mlp.fit(selected_features_normalized, labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Clusters: 25\n",
      "Average Accuracy (RBF Network): 0.49818181818181817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Clusters: 10\n",
      "Average Accuracy (RBF Network): 0.5218181818181817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Clusters: 15\n",
      "Average Accuracy (RBF Network): 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Clusters: 20\n",
      "Average Accuracy (RBF Network): 0.5145454545454545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scaler = StandardScaler()\n",
    "selected_features_normalized = scaler.fit_transform(selected_features_PSO)\n",
    "\n",
    "hidden_layer_sizes_rbf = (5, 4)  \n",
    "n_clusters = 10 \n",
    "\n",
    "best_accuracy_rbf = 0\n",
    "best_params_rbf = {}\n",
    "\n",
    "for n_clusters in [25, 10, 15,20]:  \n",
    "    rbf_pipeline = make_pipeline(\n",
    "        KMeans(n_clusters=n_clusters),\n",
    "        MLPClassifier(hidden_layer_sizes=hidden_layer_sizes_rbf, activation='identity', max_iter=1000)\n",
    "    )\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    accuracies_rbf = cross_val_score(rbf_pipeline, selected_features_normalized, train_labels, cv=kfold, scoring='accuracy')\n",
    "\n",
    "    avg_accuracy_rbf = np.mean(accuracies_rbf)\n",
    "\n",
    "    print(f\"Number of Clusters: {n_clusters}\")\n",
    "    print(f\"Average Accuracy (RBF Network): {avg_accuracy_rbf}\")\n",
    "\n",
    "    if avg_accuracy_rbf > best_accuracy_rbf:\n",
    "        best_accuracy_rbf = avg_accuracy_rbf\n",
    "        best_params_rbf['n_clusters'] = n_clusters\n",
    "\n",
    "best_rbf_pipeline = make_pipeline(\n",
    "    KMeans(n_clusters=best_params_rbf['n_clusters']),\n",
    "    MLPClassifier(hidden_layer_sizes=hidden_layer_sizes_rbf, activation='identity', max_iter=1000)\n",
    ")\n",
    "best_rbf_pipeline.fit(selected_features_normalized, train_labels)\n",
    "\n",
    "best_rbf = best_rbf_pipeline\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
